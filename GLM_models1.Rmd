---
title: "GLM - Project 707"
author: "Hannah Damico"
date: "2022-11-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(magrittr)
library(lme4)
library(DataExplorer)
library(finalfit)
library(flextable)
library(chron)
library(lubridate)
library(here)
```

```{r}
# setwd("/Users/hannahdamico/Desktop/F22/707 - Machine Learning/707FinalProject/")
```

/Users/hannahdamico/Desktop/F22/707 - Machine Learning/707FinalProject/train.csv

### Import Training and Testing Data

Repalce file path with user path.

```{r}
# Training 
train_df <- read.csv("train.csv")
# Testing
test_df <- read.csv("test.csv")
```


### Addressing Missing Data for GLM

```{r fig.height=6, fig.width=8}
missing_plot <- plot_missing(train_df)
missing_plot
```

### Tabled Missing Data

```{r}
train_glimpse <- train_df %>% ff_glimpse()
test_glimpse <- test_df %>%  ff_glimpse()
  
train_glimpse$Continuous %>%
  dplyr::select(-var_type, -n, -quartile_25, -quartile_75) %>%
  arrange(desc(missing_percent)) %>%
  flextable() %>%
  autofit() %>%
  set_caption("Continuous Missing Value Table")

# train_glimpse$Categorical
# str(train_df)
```

### Pull names of variables to remove

**DECISION ON MISSINGNESS: ** For the purpose of the Logistic GLM we will remove variables with greater than 50% missing data. These variables include "ALP", "ALT", "AST", "Albumin", "Bilirubin", "Cholesterol", "RespRate", "TroponinI", TroponinT". 


```{r}
### Training Data Subsetting
variables_kept <- train_glimpse$Continuous %>% 
  filter(as.numeric(missing_percent) <= 50.0) %>% rownames()

### Add time variable back in , only run once
 variables_kept <- c(variables_kept, "Time")
  
variables_thrown <- train_glimpse$Continuous %>% 
  filter(as.numeric(missing_percent) > 50.0) %>%  rownames()

#### Test Data Subsetting 
variables_kept_test <- test_glimpse$Continuous %>% 
  filter(as.numeric(missing_percent) <= 50.0) %>% rownames()

### Add time variable back in 
 variables_kept_test <- c(variables_kept_test, "Time")

variables_thrown_test <- test_glimpse$Continuous %>% 
  filter(as.numeric(missing_percent) > 50.0) %>%  rownames()

### Check that we kept the same variables across sets
#variables_kept == variables_kept_test

### Subset data based on missingness
subset_train <- train_df[variables_kept]
subset_test <- test_df[variables_kept]
```



### Breaking Into Intervals - Understanding our Time Variable
We will subset into 4 intervals of time and build models for each interval.

### Training Set Hour

```{r}
# converts date to type double 
# dates <- times(format(as.POSIXct(strptime(subset_train$Time, format = "%H:%M")), format = "%H:%M:%S"))

## Adds seconds to time for formatting purposes
times_hms <- hms(paste0(subset_train$Time, ":00"), quiet = TRUE)
# times_hms@hour

# Extracts the hour of a time variable
Hour <- hour(strptime(subset_train$Time, format = "%H:%M"))

# Add Hour column to subsetted train frame
subset_train %<>% 
  mutate(Hour = times_hms@hour)


```

### Testing Set Hours Intervals

```{r}

## Adds seconds to time for formatting purposes
times_hms_test <- hms(paste0(subset_test$Time, ":00"), quiet = TRUE)
# times_hms_test@hour

# Extracts the hour of a time variable
Hour_test <- hour(strptime(subset_test$Time, format = "%H:%M"))

# Add Hour column to subsetted Test frame
subset_test %<>% 
  mutate(Hour = times_hms_test@hour)

```


### Interval Assessment


### Omit Missing Values After Extracting Time

```{r}
subset_train_full <- subset_train %>% na.omit()

subset_test_full <- subset_test %>% na.omit()
```


## Interval Building

### Interval for Full Data - Including Missing Values, Train Set

```{r}
subset_train %<>% 
  mutate(Interval = case_when(between(Hour, 0, 12) ~ 1, 
                              between(Hour, 13, 23) ~ 2,
                              between(Hour, 24, 34) ~ 3,
                              between(Hour, 35, 48) ~ 4)) 

### Number of Unique Subject IDs across intervals
uniq_ids <- subset_train %>% 
  select(subjid, Interval) %>% 
  group_by(Interval) %>% 
  summarize(Unique_Subjid = length(unique(subjid)))

# length(unique(subset_train$subjid))
# length(unique(subset_train_full$subjid))
```



### Interval for Full Data - Including Missing Values, Test Set

```{r}
subset_test %<>% 
  mutate(Interval = case_when(between(Hour, 0, 12) ~ 1, 
                              between(Hour, 13, 23) ~ 2,
                              between(Hour, 24, 34) ~ 3,
                              between(Hour, 35, 48) ~ 4)) 

### Number of Unique Subject IDs across intervals
uniq_ids <- subset_test %>% 
  select(subjid, Interval) %>% 
  group_by(Interval) %>% 
  summarize(Unique_Subjid = length(unique(subjid)))

```



### Interval for Reduced Data - Excluding Missing Values

```{r}
subset_train_full %<>% 
  mutate(Interval = case_when(between(Hour, 0, 12) ~ 1, 
                              between(Hour, 13, 23) ~ 2,
                              between(Hour, 24, 34) ~ 3,
                              between(Hour, 35, 48) ~ 4)) 

### Number of Unique Subject IDs across intervals
unq_id_omit <- subset_train_full %>% 
  select(subjid, Interval) %>% 
  group_by(Interval) %>% 
  summarize(Unique_Subjid_Omit = length(unique(subjid)))


## Gives the % of unique patient data after omitting NAs
merged_uniques <- left_join(unq_id_omit, uniq_ids, by = "Interval") %>% group_by(Interval) %>% summarize(Unique_Subjid, Unique_Subjid_Omit, Percentage = round(Unique_Subjid_Omit / Unique_Subjid*100,3))

## Gives number of NA per column per interval
## Makes sense that I1 would have most missing 
## Lab Values were likely not taken in the first 12 hours of admission
subset_train %>%
  group_by(Interval) %>% 
  select(-Time, -X) %>% 
  summarise_at(vars(-subjid), "is.na") %>% 
  summarise_all("sum")

```

### Mean Lab Value per Interval per Patient

Taking the mean per interval per subject ID reduces the data by excluding all NA values and maintaining only 1 row of data per subject per interval.

```{r}
## Take the mean of lab value per subject ID per Interval
mean_values_frame <- subset_train %>% 
  na.omit() %>% 
  group_by(Interval, subjid) %>% 
  select(-Time, -X) %>% 
  summarise_all("mean")
```



## Addressing Missing Data for GLM

For the purpose of this project, we will use the subset_train and subset_test data and impute the missing values in these datasets. This decision was made since we evaluated that the percentage of data lost when simply removing missing values was too substantial and losing this amount of information was too costly for our modeling.

We will decide to impute the data both upwards and downwards per subject ID per time interval. This means that if a subject is missing data for the first 4 time points of interval 2 for the variable BUN, then the these time points will be imputed using the value recorded at the 5th time point. In other cases, time points per subject ID and time interval will need to be imputed downwards.

### Imputing Missing Data - Training set

```{r}
lab_cols_train <- colnames(subset_train)[3:39]

# Fill/Impute Values upward then downward

subset_train <- subset_train %>% 
  group_by(subjid) %>% 
  tidyr::fill(lab_cols_train, .direction = c("updown"))


```


```{r}
## Function to find mean of variable and replace NAs with that mean
impute_mean <- function(variable){
  variable <- ifelse(is.na(variable), mean(variable,na.rm=TRUE), variable)
  return(variable)
}
```


```{r}
## Apply function to columns of data grouped by subjid & Interval
subset_train %<>% # CAUTION: the %<>% operator will save over your existing df, revert to %>% 
  group_by(subjid, Interval)  %>% 
  summarise_at(vars(-X), "impute_mean")

# Uncomment this code and run the above lines to see change in count of missing data
# %>% 
#   summarise_all("is.na") %>% 
#   summarise_all("sum")
```

### Imputing Missing Data - Testing set

```{r}
lab_cols_test <- colnames(subset_test)[3:38]

# Fill/Impute Values upward then downward

subset_test <- subset_test %>% 
  group_by(subjid) %>% 
  tidyr::fill(lab_cols_test, .direction = c("updown"))


```

```{r}
## Apply function to columns of data grouped by subjid & Interval
subset_test %<>% # CAUTION: the %<>% operator will save over your existing df, revert to %>% 
  group_by(subjid, Interval)  %>% 
  summarise_at(vars(-X), "impute_mean")

# Uncomment this code and run the above lines to see change in count of missing data
# %>% 
#   summarise_all("is.na") %>% 
#   summarise_all("sum")
```



### Check Count of Missing Data

```{r}
subset_train %>%
  group_by(Interval) %>% 
  select(-Time) %>% 
  summarise_at(vars(-subjid), "is.na") %>% 
  summarise_all("sum")

subset_test %>%
  group_by(Interval) %>% 
  select(-Time) %>% 
  summarise_at(vars(-subjid), "is.na") %>% 
  summarise_all("sum")
```




```{r fig.width=4, fig.height=4}
plot_missing(subset_train)
plot_missing(subset_test)
```

### Write .csv Files of the imputed data

```{r}
# write.csv(subset_train, "imputed_train.csv")
# write.csv(subset_test, "imputed_test.csv")
```


### Fit a Saturated Logistic GLM  
This saturated model will help to identify variables that are significant with respect to our primary outcome of in hospital death. This model is fit with data that contains NA values, but opts to remove them in order to fit the model.

```{r}
saturated_model <- glmer(in_hosp_death ~ . -Time - MechVent - MechVent_cleaned + (1 + Interval | subjid), data = subset_train,family = binomial(link = "logit"), na.action = na.omit)

summary(saturated_model)
```

