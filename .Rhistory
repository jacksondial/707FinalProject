print(conf_mat)
roc.test = roc(test_data_bl$in_hosp_death, as.numeric(pred_y))
rfm_auc = auc(roc.test)
# Plot of Specificity vs. Sensitivity
plot(roc.test)
rfm_auc
# Plot of Specificity vs. Sensitivity
plot(roc.test)
# Plot of Specificity vs. Sensitivity
plot(roc.test, colorize = TRUE)
cfm
knitr::opts_chunk$set(echo = TRUE)
# Read in the data
data = read.csv("table8_5.csv")
str(data)
View(data)
# Read in the data
data = read.csv("table8_5.csv")
View(data)
# Nominal model of just housing condition
nominal.fit1 = vglm(cbind(satisfaction) ~ type, family = multinominal, data = data)
# Libraries
library(VGAM)
# Nominal model of just housing condition
nominal.fit1 = vglm(cbind(satisfaction) ~ type, family = multinominal, data = data)
?vglm
# Nominal model of just housing condition
nominal.fit1 = vglm(cbind(satisfaction) ~ type, family = multinomial, data = data)
summary(nominal.fit1)
# Nominal model of just contact level with other residents
nominal.fit2 = vglm(cbind(satisfaction) ~ contact, family = multinomial, data = data)
summary(nominal.fit2)
# Nominal model of contact level and housing type and interaction of thw two variables
nominal.fit3 = vglm(cbind(satisfaction) ~ type + contact +type*contact, family = multinomial, data = data)
summary(nominal.fit3)
# Nominal model of just contact level with other residents
nominal.fit2 = vglm(satisfaction ~ contact, family = multinomial, data = data)
summary(nominal.fit2)
# Nominal model of just contact level with other residents
nominal.fit2 = multinomial(satisfaction ~ contact, family = multinomial, data = data)
# Nominal model of just contact level with other residents
nominal.fit2 = multinomial(satisfaction ~ contact, data = data)
# Nominal model of just contact level with other residents
nominal.fit2 = multinomial(satisfaction ~ contact, data)
summary(nominal.fit2)
# Nominal model of just contact level with other residents
nominal.fit2 = vglm(satisfaction ~ contact, family = multinomial, data = data)
summary(nominal.fit2)
# Libraries
library(randomForest)
library(ROCR)
library(pROC)
library(ggRandomForests)
library(LongituRF)
library(xgboost)
library(caret)
library(tidyverse)
library(ggplot2)
library(Matrix)
library(Ckmeans.1d.dp)
library(DataExplorer)
library(dplyr)
# Read in data
train_data = read.csv("/Users/KovicFamily/Documents/Duke MB/Courses/2022-2023 Courses/Fall 2022/BIOSTAT 707 - Stat Machine Learning/Final Project/707FinalProject/train.csv") %>%
select(-1) %>% # Remove the extra index column
rename(Height = height_cleaned) # rename height variable
test_data = read.csv("/Users/KovicFamily/Documents/Duke MB/Courses/2022-2023 Courses/Fall 2022/BIOSTAT 707 - Stat Machine Learning/Final Project/707FinalProject/test.csv") %>%
select(-1) %>% # Remove the extra index column
rename(Height = height_cleaned) # rename height variable
names(train_data)
# Make in_hosp_death and Gender factors to fit a classification RF model
train_data$in_hosp_death = as.factor(train_data$in_hosp_death)
train_data$Gender = as.factor(train_data$Gender)
test_data$in_hosp_death = as.factor(test_data$in_hosp_death)
test_data$Gender = as.factor(test_data$Gender)
train_data$Weight
# Make in_hosp_death and Gender factors to fit a classification RF model
train_data$in_hosp_death = as.factor(train_data$in_hosp_death)
train_data$Gender = as.factor(train_data$Gender)
test_data$in_hosp_death = as.factor(test_data$in_hosp_death)
test_data$Gender = as.factor(test_data$Gender)
# Libraries
library(randomForest)
library(ROCR)
library(pROC)
library(ggRandomForests)
library(LongituRF)
library(xgboost)
library(caret)
library(tidyverse)
library(ggplot2)
library(Matrix)
library(Ckmeans.1d.dp)
library(DataExplorer)
library(dplyr)
names(train_data)
train_data_bl = train_data %>%
dplyr::select(-c("subjid")) %>% # remove subjid for modeling purposes
dplyr::select(Age, Gender, Height, Weight, ICUType, in_hosp_death)
test_data_bl = test_data %>%
dplyr::select(-c("subjid")) %>%
dplyr::select(Age, Gender, Height, Weight, ICUType, in_hosp_death)
train_data_bl = train_data %>%
dplyr::select(-c("subjid")) %>% # remove subjid for modeling purposes
dplyr::select(Age, Gender, Height, Weight, ICUType, in_hosp_death) %>%
mutate(BMI = Weight/(Height/100)^2)
train_data_bl = train_data %>%
dplyr::select(-c("subjid")) %>% # remove subjid for modeling purposes
dplyr::select(Age, Gender, Height, Weight, ICUType, in_hosp_death) %>%
mutate(BMI = Weight/((Height/100)^2))
test_data_bl = test_data %>%
dplyr::select(-c("subjid")) %>%
dplyr::select(Age, Gender, Height, Weight, ICUType, in_hosp_death) %>%
mutate(BMI = Weight/((Height/100)^2))
summary(train_data_bl$BMI
summary(train_data_bl$BMI)
summary(train_data_bl$BMI)
train_data_bl %>%
ggplot(aes(x = in_hosp_death, y = BMI)) +
geom_bar()
train_data_bl %>%
ggplot(aes(x = in_hosp_death, weight = BMI)) +
geom_bar()
train_data_bl %>%
ggplot(aes(x = in_hosp_death, y = BMI)) +
geom_bar(stat = identity)
train_data_bl %>%
ggplot(aes(x = in_hosp_death, y = BMI)) +
geom_bar(stat = "identity")
train_data_bl %>%
ggplot(aes(x = in_hosp_death, y = BMI)) +
geom_bar(stat = 'identity')
train_data_bl %>%
ggplot(aes(x = BMI, y = in_hosp_death)) +
geom_bar(stat = 'identity')
# Classification Random Forest model using baseline features
set.seed(11022009)
rfm_bl = randomForest(in_hosp_death ~ ., data = train_data_bl, ntree = 500)
rfm_bl = randomForest(in_hosp_death ~ ., data = train_data_bl, ntree = 500)
print(rfm_bl)
# Read in data
train_data = read.csv("/Users/KovicFamily/Documents/Duke MB/Courses/2022-2023 Courses/Fall 2022/BIOSTAT 707 - Stat Machine Learning/Final Project/707FinalProject/train.csv") %>%
select(-1) %>% # Remove the extra index column
rename(Height = height_cleaned) # rename height variable
# Read in data
train_data = read.csv("/Users/KovicFamily/Documents/Duke MB/Courses/2022-2023 Courses/Fall 2022/BIOSTAT 707 - Stat Machine Learning/Final Project/707FinalProject/train.csv") %>%
select(-1) %>% # Remove the extra index column
rename(Height = height_cleaned) # rename height variable
test_data = read.csv("/Users/KovicFamily/Documents/Duke MB/Courses/2022-2023 Courses/Fall 2022/BIOSTAT 707 - Stat Machine Learning/Final Project/707FinalProject/test.csv") %>%
select(-1) %>% # Remove the extra index column
rename(Height = height_cleaned) # rename height variable
test_data$Height = (test_data$Height)/100
train_data$Height
summary(train_data$Height)
# Read in data
train_data = read.csv("/Users/KovicFamily/Documents/Duke MB/Courses/2022-2023 Courses/Fall 2022/BIOSTAT 707 - Stat Machine Learning/Final Project/707FinalProject/train.csv") %>%
select(-1) %>% # Remove the extra index column
rename(Height = height_cleaned) # rename height variable
summary(train_data$Height)
# Read in data
train_data = read.csv("/Users/KovicFamily/Documents/Duke MB/Courses/2022-2023 Courses/Fall 2022/BIOSTAT 707 - Stat Machine Learning/Final Project/707FinalProject/train.csv") %>%
select(-1) %>% # Remove the extra index column
rename(Height = height_cleaned) # rename height variable
# Make in_hosp_death and Gender factors to fit a classification RF model
train_data$in_hosp_death = as.factor(train_data$in_hosp_death)
train_data$Gender = as.factor(train_data$Gender)
train_data$Height = (train_data$Height)/100 # convert Height from cm to meters
summary(train_data$Height)
test_data$in_hosp_death = as.factor(test_data$in_hosp_death)
test_data$Gender = as.factor(test_data$Gender)
# convert Height from cm to meters
test_data$Height = (test_data$Height)/100
train_data_bl = train_data %>%
dplyr::select(-c("subjid")) %>% # remove subjid for modeling purposes
dplyr::select(Age, Gender, Height, Weight, ICUType, in_hosp_death) %>%
mutate(BMI = Weight/((Height)^2)) # Make BMI variable
# Read in data
train_data = read.csv("/Users/KovicFamily/Documents/Duke MB/Courses/2022-2023 Courses/Fall 2022/BIOSTAT 707 - Stat Machine Learning/Final Project/707FinalProject/train.csv") %>%
select(-1) %>% # Remove the extra index column
rename(Height = height_cleaned) # rename height variable
test_data = read.csv("/Users/KovicFamily/Documents/Duke MB/Courses/2022-2023 Courses/Fall 2022/BIOSTAT 707 - Stat Machine Learning/Final Project/707FinalProject/test.csv") %>%
select(-1) %>% # Remove the extra index column
rename(Height = height_cleaned) # rename height variable
# convert Height from cm to meters
train_data$Height_m = (train_data$Height)/100
# convert Height from cm to meters
test_data$Height_m = (test_data$Height)/100
# Make in_hosp_death and Gender factors to fit a classification RF model
train_data$in_hosp_death = as.factor(train_data$in_hosp_death)
train_data$Gender = as.factor(train_data$Gender)
test_data$in_hosp_death = as.factor(test_data$in_hosp_death)
test_data$Gender = as.factor(test_data$Gender)
summary(train_data$Height_m)
train_data_bl = train_data %>%
dplyr::select(-c("subjid")) %>% # remove subjid for modeling purposes
dplyr::select(Age, Gender, Height_m, Weight, ICUType, in_hosp_death) %>%
mutate(BMI = Weight/((Height_m)^2)) # Make BMI variable
test_data_bl = test_data %>%
dplyr::select(-c("subjid")) %>%
dplyr::select(Age, Gender, Height_m, Weight, ICUType, in_hosp_death) %>%
mutate(BMI = Weight/((Height_m)^2)) # Make BMI variable
# Classification Random Forest model using baseline features
set.seed(11022009)
rfm_bl = randomForest(in_hosp_death ~ ., data = train_data_bl, ntree = 500)
rfm_bl = randomForest(in_hosp_death ~ ., data = train_data_bl, ntree = 500)
print(rfm_bl)
# Libraries
library(randomForest)
library(ROCR)
library(pROC)
library(ggRandomForests)
library(LongituRF)
library(xgboost)
library(caret)
library(tidyverse)
library(ggplot2)
library(Matrix)
library(Ckmeans.1d.dp)
library(DataExplorer)
library(dplyr)
# Read in data
train_data = read.csv("/Users/KovicFamily/Documents/Duke MB/Courses/2022-2023 Courses/Fall 2022/BIOSTAT 707 - Stat Machine Learning/Final Project/707FinalProject/train.csv") %>%
select(-1) %>% # Remove the extra index column
rename(Height = height_cleaned) # rename height variable
test_data = read.csv("/Users/KovicFamily/Documents/Duke MB/Courses/2022-2023 Courses/Fall 2022/BIOSTAT 707 - Stat Machine Learning/Final Project/707FinalProject/test.csv") %>%
select(-1) %>% # Remove the extra index column
rename(Height = height_cleaned) # rename height variable
train_data$Height = (train_data$Height)/100
test_data$Height = (test_data$Height)/100
# Read in data
train_data = read.csv("/Users/KovicFamily/Documents/Duke MB/Courses/2022-2023 Courses/Fall 2022/BIOSTAT 707 - Stat Machine Learning/Final Project/707FinalProject/train.csv") %>%
select(-1) %>% # Remove the extra index column
rename(Height = height_cleaned) # rename height variable
test_data = read.csv("/Users/KovicFamily/Documents/Duke MB/Courses/2022-2023 Courses/Fall 2022/BIOSTAT 707 - Stat Machine Learning/Final Project/707FinalProject/test.csv") %>%
select(-1) %>% # Remove the extra index column
rename(Height = height_cleaned) # rename height variable
# Make in_hosp_death and Gender factors to fit a classification RF model
train_data$in_hosp_death = as.factor(train_data$in_hosp_death)
train_data$Gender = as.factor(train_data$Gender)
train_data$Height_m = (train_data$Height)/100
test_data$in_hosp_death = as.factor(test_data$in_hosp_death)
test_data$Gender = as.factor(test_data$Gender)
test_data$Height_m = (test_data$Height)/100
train_data_bl = train_data %>%
dplyr::select(-c("subjid")) %>% # remove subjid for modeling purposes
dplyr::select(Age, Gender, Height_m, Weight, ICUType, in_hosp_death) %>%
mutate(BMI = Weight/(Height_m^2))
test_data_bl = test_data %>%
dplyr::select(-c("subjid")) %>%
dplyr::select(Age, Gender, Height_m, Weight, ICUType, in_hosp_death) %>%
mutate(BMI = Weight/(Height_m^2))
# Classification Random Forest model using baseline features
set.seed(11022009)
rfm_bl = randomForest(in_hosp_death ~ ., data = train_data_bl, ntree = 500)
rfm_bl = randomForest(in_hosp_death ~ ., data = train_data_bl, ntree = 500)
print(rfm_bl)
train_data_bl = train_data %>%
dplyr::select(-c("subjid")) %>% # remove subjid for modeling purposes
dplyr::select(Age, Gender, Height_m, Weight, ICUType, in_hosp_death) %>%
mutate(BMI = Weight/(Height_m^2)) %>%
subset(select = -c(Age, Gender, BMI, ICUType, in_hosp_death))
names(train_data_bl)
train_data_bl = train_data %>%
dplyr::select(-c("subjid")) %>% # remove subjid for modeling purposes
dplyr::select(Age, Gender, Height_m, Weight, ICUType, in_hosp_death) %>%
mutate(BMI = Weight/(Height_m^2)) %>%
subset(select = -c(Weight, Height))
train_data_bl = train_data %>%
dplyr::select(-c("subjid")) %>% # remove subjid for modeling purposes
dplyr::select(Age, Gender, Height_m, Weight, ICUType, in_hosp_death) %>%
mutate(BMI = Weight/(Height_m^2)) %>%
subset(select = -c(Weight, Height_m))
names(train_data_bl)
test_data_bl = test_data %>%
dplyr::select(-c("subjid")) %>%
dplyr::select(Age, Gender, Height_m, Weight, ICUType, in_hosp_death) %>%
mutate(BMI = Weight/(Height_m^2)) %>%
subset(select = -c(Weight, Height_m)) # Remove Weight and Height from test data
# Classification Random Forest model using baseline features
set.seed(11022009)
rfm_bl = randomForest(in_hosp_death ~ ., data = train_data_bl, ntree = 500)
print(rfm_bl)
# Evaluate prediction accuracy
rfm_pred = predict(rfm_bl, test_data_bl)
cfm
# Build Confusion Matrix
cfm = table(test_data_bl$in_hosp_death, rfm_pred, exclude = NULL)
cfm
# Classification accuracy
classification_accuracy = sum(diag(cfm)/sum(cfm))
classification_accuracy
# Compute AUC of RF Model
roc.test = roc(test_data_bl$in_hosp_death, as.numeric(rfm_pred))
rfm_auc = auc(roc.test)
# Plot of Specificity vs. Sensitivity
plot(roc.test)
rfm_auc
# Create dataframe for variable importance plot
var_importance = data_frame(variable = setdiff(colnames(train_data_bl), "in_hosp_death"),
importance = as.vector(importance(rfm_bl)))
# Arrange Age, Gender, height and ICU type in order of most to lenast important
var_importance = var_importance %>%
arrange(desc(importance))
# Make the variable in var_importance a factor for plotting purposes
var_importance$variable = factor(var_importance$variable,
levels = var_importance$variable)
# RF Variable Importance plot
p = ggplot(var_importance, aes(x=variable, weight=importance, fill=variable))
p = p + geom_bar() + ggtitle("Variable Importance from Random Forest of Baseline Features")
p = p + xlab("Demographic Attribute") + ylab("Variable Importance (Mean Decrease in Gini Index)")
p = p + scale_fill_discrete(name="Variable Name")
p + theme(axis.text.x = element_blank(),
axis.text.y=element_text(size=12),
axis.title=element_text(size=16),
plot.title=element_text(size=18),
legend.title=element_text(size=16),
legend.text=element_text(size=12))
# Independent variables for train
X_train = data.matrix(train_data_bl[, -5])
# Dependent variable for train
y_train = as.numeric(as.character(train_data_bl[, 5])) # Must make the y_train a character and then numeric
# Dependent variable for test
y_test = as.numeric(as.character(test_data_bl[, 5])) # Must make y_test a character and then numeric
# Convert the train and test data into xgboost matrix type
xgboost_train = xgb.DMatrix(data = X_train, label = y_train)
xgboost_test = xgb.DMatrix(data = X_test, label = y_test)
# Independent variables for test
X_test = data.matrix(test_data_bl[, -5])
# Dependent variable for test
y_test = as.numeric(as.character(test_data_bl[, 5])) # Must make y_test a character and then numeric
# Convert the train and test data into xgboost matrix type
xgboost_train = xgb.DMatrix(data = X_train, label = y_train)
xgboost_test = xgb.DMatrix(data = X_test, label = y_test)
set.seed(11022009) # using seed that Jackson used to generate training and testing data
# XGBoost Model #
# Train an xgboost model using our training data - outputs training error
xgb_bl = xgboost(data = xgboost_train, # independent variables training data
eta = 1,
max.depth = 4,
nthread = 2,
nrounds = 10, # Should this be increased or decreased ?
objective = "binary:logistic", # classification XGBoost
eval_metric = "auc") # AUC
# Independent variables for train
X_train = data.matrix(train_data_bl[, -5])
# Dependent variable for train
y_train = as.numeric(as.character(train_data_bl[, 5])) # Must make the y_train a character and then numeric
# Independent variables for test
X_test = data.matrix(test_data_bl[, -5])
# Dependent variable for test
y_test = as.numeric(as.character(test_data_bl[, 5])) # Must make y_test a character and then numeric
# Convert the train and test data into xgboost matrix type
xgboost_train = xgb.DMatrix(data = X_train, label = y_train)
xgboost_test = xgb.DMatrix(data = X_test, label = y_test)
set.seed(11022009) # using seed that Jackson used to generate training and testing data
# XGBoost Model #
# Train an xgboost model using our training data - outputs training error
xgb_bl = xgboost(data = xgboost_train, # independent variables training data
eta = 1,
max.depth = 4,
nthread = 2,
nrounds = 10, # Should this be increased or decreased ?
objective = "binary:logistic", # classification XGBoost
eval_metric = "auc") # AUC
# Independent variables for train
X_train = data.matrix(train_data_bl[, -"in_hosp_death"])
# Independent variables for train
X_train = data.matrix(train_data_bl[, -in_hosp_death])
train_data_bl[, -in_hosp_death]
train_data_bl[, 5]
names(train_data_bl)
train_data_bl[, 4]
# Independent variables for train
X_train = data.matrix(train_data_bl[, -4])
# Dependent variable for train
y_train = as.numeric(as.character(train_data_bl[, 4])) # Must make the y_train a character and then numeric
# Independent variables for test
X_test = data.matrix(test_data_bl[, -4])
# Dependent variable for test
y_test = as.numeric(as.character(test_data_bl[, 4])) # Must make y_test a character and then numeric
# Convert the train and test data into xgboost matrix type
xgboost_train = xgb.DMatrix(data = X_train, label = y_train)
xgboost_test = xgb.DMatrix(data = X_test, label = y_test)
set.seed(11022009) # using seed that Jackson used to generate training and testing data
# XGBoost Model #
# Train an xgboost model using our training data - outputs training error
xgb_bl = xgboost(data = xgboost_train, # independent variables training data
eta = 1,
max.depth = 4,
nthread = 2,
nrounds = 10, # Should this be increased or decreased ?
objective = "binary:logistic", # classification XGBoost
eval_metric = "auc") # AUC
# Summary of XGBoost model
summary(xgb_bl)
# Use model to get predictions on test data in terms of probabilities
pred_test = predict(xgb_bl, xgboost_test)
# Convert probability predictions to 0's and 1's
pred_y = as.numeric(pred_test > 0.5) # variable -- Any probability > 0.5 is classified as 1 and 0, otherwise
# Confusion matrix -- need to convert y_test and pred_y to factors first
conf_mat = confusionMatrix(factor(y_test), factor(pred_y))
# Calculate AUC and plot AUC curve for XGBoost
roc.test = roc(test_data_bl$in_hosp_death, as.numeric(pred_y))
rfm_auc = auc(roc.test)
# Plot of Specificity vs. Sensitivity
plot(roc.test, colorize = TRUE, title = "AUC Curve of XGBoost Model")
# Variable Importance Plot for XGBoost
importance_xgb = xgb.importance(colnames(xgboost_train), model = xgb_bl)
xgb_imp_plot = xgb.ggplot.importance(importance_xgb, top_n = 4)
xgb_imp_plot + theme(legend.position = "none",
axis.text.y=element_text(size=12),
axis.title=element_text(size=16),
plot.title=element_text(size=18),
legend.title=element_text(size=16),
legend.text=element_text(size=12)) +
ggtitle("Feature Importance for XGBoost")
# Output importance matrix for XGBoost
print(importance_xgb)
roc.test
?roc
roc(test_data_bl$in_hosp_death, as.numeric(rfm_pred), plot = TRUE)
roc(test_data_bl$in_hosp_death, as.numeric(rfm_pred), plot = TRUE, percent = TRUE)
roc(test_data_bl$in_hosp_death, as.numeric(rfm_pred), plot = TRUE, title = "AUROC Curve for RF Model at Baseline")
plot(roc.test)
roc.test
roc.test
roc.test
rfm_auc = auc(roc.test)
print(rfm_auc)
roc.test
# Compute AUC of RF Model
roc.test = roc(test_data_bl$in_hosp_death, as.numeric(rfm_pred), plot = TRUE, auc = FALSE)
roc.test
# Compute AUC of RF Model
roc_obj = roc(test_data_bl$in_hosp_death, as.numeric(rfm_pred), plot = TRUE)
roc_obj # outputs AUROC
set.seed(11022009)
# Compute AUC of RF Model
roc_obj = roc(test_data_bl$in_hosp_death, as.numeric(rfm_pred), plot = TRUE)
roc_obj # outputs AUROC values
# Classification Random Forest model using baseline features
# BMI was used in place of Height and Weight
set.seed(11022009)
rfm_bl = randomForest(in_hosp_death ~ ., data = train_data_bl, ntree = 500)
rfm_bl = randomForest(in_hosp_death ~ ., data = train_data_bl, ntree = 500)
print(rfm_bl)
print(rfm_bl)
set.seed(11022009)
# Evaluate prediction accuracy
rfm_pred = predict(rfm_bl, test_data_bl)
classification_accuracy
set.seed(11022009)
# Compute AUC of RF Model
roc_obj = roc(test_data_bl$in_hosp_death, as.numeric(rfm_pred), plot = TRUE)
roc_obj # outputs AUROC values
set.seed(11022009)
# Compute AUC of RF Model
roc_obj = roc(test_data_bl$in_hosp_death, as.numeric(rfm_pred), plot = TRUE)
roc_obj # outputs AUROC values
ggroc(roc.test)
rfm_auc = auc(roc_obj)
print(rfm_auc)
auroc = round(auc(roc_obj), 4)
# Plot of ROC Curve
ggroc(roc_obj, color = "steelblue", size = 2) +
ggtitle(paste0('ROC Curve ', '(AUC = ', auroc, ')')) +
theme_bw()
# Plot of ROC Curve
ggroc(roc_obj, color = "steelblue", size = 1.7) +
ggtitle(paste0('ROC Curve ', '(AUC = ', auroc, ')')) +
theme_bw()
?ggroc
# Plot of ROC Curve
ggroc(roc_obj, color = "steelblue", size = 1.7) +
ggtitle(paste0('ROC Curve for Random Forest', '(AUC = ', auroc, ')')) +
theme_bw()
# Plot of ROC Curve
ggroc(roc_obj, color = "steelblue", size = 1.7) +
ggtitle(paste0('ROC Curve for Random Forest ', '(AUC = ', auroc, ')')) +
theme_bw()
# Plot of ROC Curve
ggroc(roc_obj, color = "steelblue", size = 1.7) +
ggtitle(paste0('ROC Curve for Random Forest at Baseline ', '(AUC = ', auroc, ')')) +
theme_bw()
set.seed(11022009)
# Use model to get predictions on test data in terms of probabilities
xgb_pred_test = predict(xgb_bl, xgboost_test)
# Convert probability predictions to 0's and 1's
xgb_pred = as.numeric(pred_test > 0.5) # variable -- Any probability > 0.5 is classified as 1 and 0, otherwise
# Confusion matrix -- need to convert y_test and pred_y to factors first
conf_mat = confusionMatrix(factor(y_test), factor(xgb_pred))
print(conf_mat)
auroc_xgb = round(auc(roc_obj_xgb), 4) # AUROC
# Calculate AUC and plot AUC curve for XGBoost
roc_obj_xgb = roc(test_data_bl$in_hosp_death, as.numeric(pred_y))
auroc_xgb = round(auc(roc_obj_xgb), 4) # AUROC
# Plot of Specificity vs. Sensitivity
ggroc(roc_obj_xgb, color = "steelblue", size = 1.7) +
ggtitle(paste0('ROC Curve for XGBoost Model at Baseline ', '(AUC = ', auroc, ')')) +
theme_bw()
# Plot of Specificity vs. Sensitivity
ggroc(roc_obj_xgb, color = "steelblue", size = 1) +
ggtitle(paste0('ROC Curve for XGBoost Model at Baseline ', '(AUC = ', auroc, ')')) +
theme_bw()
# Plot of Specificity vs. Sensitivity
ggroc(roc_obj_xgb, color = "steelblue", size = 1.7) +
ggtitle(paste0('ROC Curve for XGBoost Model at Baseline ', '(AUC = ', auroc, ')')) +
theme_bw()
# Plot of Specificity vs. Sensitivity
ggroc(roc_obj_xgb, color = "steelblue", size = 1.7) +
ggtitle(paste0('ROC Curve for XGBoost Model at Baseline ', '(AUC = ', auroc_xgb, ')')) +
theme_bw()
